import asyncio
import re
from bs4 import BeautifulSoup
from typing import List, Optional
from datetime import datetime
from urllib.parse import urljoin
import logging

from models.schemas import NewsSource
from services.base_parser import BaseNewsParser

logger = logging.getLogger(__name__)

class RiaParser(BaseNewsParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è ria.ru (—Ä–∞–∑–¥–µ–ª –∑–¥–æ—Ä–æ–≤—å–µ)"""
    
    def __init__(self):
        super().__init__("ria", "https://ria.ru")
        self.name = "ria"  # –î–æ–±–∞–≤–ª—è–µ–º –∞—Ç—Ä–∏–±—É—Ç name
        self.news_url = "https://ria.ru/health/"
    
    async def parse_news_list(self, max_articles: int = 10, date_filter: Optional[str] = None, fetch_full_content: bool = True) -> List[NewsSource]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å–ø–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Ä–∞–∑–¥–µ–ª–∞ –∑–¥–æ—Ä–æ–≤—å–µ ria.ru —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π AJAX –ø–∞–≥–∏–Ω–∞—Ü–∏–∏"""
        try:
            news_items = []
            seen_urls = set()
            seen_titles = set()
            current_url = self.news_url
            page_num = 1
            
            while len(news_items) < max_articles:
                logger.info(f"Parsing RIA page {page_num}, current articles: {len(news_items)}")
                
                # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
                async with self.session.get(current_url) as response:
                    if response.status != 200:
                        logger.error(f"Failed to fetch news list from {current_url}: {response.status}")
                        break
                    
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –†–ò–ê
                    news_containers = soup.find_all('div', class_='list-item__content')
                    
                    if not news_containers:
                        logger.info(f"No news containers found on page {page_num}")
                        break
                    
                    logger.info(f"Found {len(news_containers)} news containers on page {page_num}")
                    
                    # –ü–∞—Ä—Å–∏–º –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                    page_added = 0
                    for container in news_containers:
                        try:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç
                            if len(news_items) >= max_articles:
                                break
                            
                            # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç–∞—Ç—å—é
                            title_link = container.find('a', class_='list-item__title')
                            if not title_link:
                                continue
                                
                            href = title_link.get('href')
                            if not href:
                                continue
                                
                            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—É—é —Å—Å—ã–ª–∫—É
                            if href.startswith('/'):
                                full_url = urljoin(self.base_url, href)
                            else:
                                full_url = href
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏ –ø–æ URL
                            if full_url in seen_urls:
                                continue
                            seen_urls.add(full_url)
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                            title = title_link.get_text(strip=True)
                            if not title:
                                continue
                                
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
                            if title in seen_titles:
                                continue
                            seen_titles.add(title)
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)
                            image_link = container.find('a', class_='list-item__image')
                            image_url = None
                            if image_link:
                                img_tag = image_link.find('img')
                                if img_tag:
                                    image_url = img_tag.get('src')
                            
                            # –ï—Å–ª–∏ –Ω—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –ø–∞—Ä—Å–∏–º —Å—Ç–∞—Ç—å—é —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
                            full_content = ""
                            published_date = None
                            published_time = None
                            views_count = None
                            author = None
                            
                            if fetch_full_content:
                                full_content, published_date, published_time, views_count, author = await self._fetch_full_article_with_metadata(full_url)
                            
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∫–∞–∫ –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –µ—Å–ª–∏ –Ω–µ—Ç –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                            content = full_content if full_content else title
                            
                            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
                            news_item = NewsSource(
                                title=title,
                                url=full_url,
                                content=content,
                                published_date=published_date,
                                published_time=published_time,
                                views_count=views_count,
                                author=author,
                                source_site="ria.ru"
                            )
                            
                            if self._is_relevant_news(news_item, date_filter):
                                news_items.append(news_item)
                                page_added += 1
                                logger.info(f"Added news {len(news_items)}: {title[:50]}...")
                                
                                # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                                await asyncio.sleep(0.5)
                                
                        except Exception as e:
                            logger.warning(f"Error parsing article container: {e}")
                            continue
                    
                    logger.info(f"Added {page_added} articles from page {page_num}")
                    
                    # –ï—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏–º–∏—Ç–∞, –≤—ã—Ö–æ–¥–∏–º
                    if len(news_items) >= max_articles:
                        break
                    
                    # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É (–∫–Ω–æ–ø–∫–∞ "–ï—â–µ" –∏–ª–∏ data-next-url)
                    data_url = None
                    
                    # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –∫–Ω–æ–ø–∫—É "–ï—â–µ"
                    more_button = soup.find('div', class_='list-more')
                    if more_button:
                        data_url = more_button.get('data-url')
                        logger.info("Found 'more' button with data-url")
                    
                    # –ï—Å–ª–∏ –∫–Ω–æ–ø–∫–∏ –Ω–µ—Ç, –∏—â–µ–º data-next-url –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ
                    if not data_url:
                        next_url_container = soup.find(attrs={'data-next-url': True})
                        if next_url_container:
                            data_url = next_url_container.get('data-next-url')
                            logger.info("Found data-next-url in container")
                    
                    if not data_url:
                        logger.info("No pagination URL found, stopping pagination")
                        break
                    
                    # –§–æ—Ä–º–∏—Ä—É–µ–º URL —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    if data_url.startswith('/'):
                        current_url = urljoin(self.base_url, data_url)
                    else:
                        current_url = data_url
                    
                    logger.info(f"Next page URL: {current_url}")
                    page_num += 1
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
                    await asyncio.sleep(1.0)
            
            logger.info(f"Successfully parsed {len(news_items)} unique news articles from RIA (pages: {page_num})")
            return news_items
                
        except Exception as e:
            logger.error(f"Error parsing RIA news list: {e}")
            return []
    
    async def _fetch_full_article(self, url: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç–∞—Ç—å–∏ —Å ria.ru"""
        try:
            async with self.session.get(url) as response:
                if response.status != 200:
                    logger.warning(f"Failed to fetch article {url}: {response.status}")
                    return ""
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe']):
                    element.decompose()
                
                # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏ –†–ò–ê
                content_parts = []
                
                # –û—Å–Ω–æ–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –†–ò–ê
                content_selectors = [
                    'div.article__body',
                    'div.article__text', 
                    'div.article-text',
                    'div[data-type="text"]',
                    '.article__content',
                    '.article-body',
                    '.article__block[data-type="text"]'
                ]
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º set –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
                seen_texts = set()
                
                for selector in content_selectors:
                    elements = soup.select(selector)
                    if elements:
                        for elem in elements:
                            # –ò—â–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤–Ω—É—Ç—Ä–∏ —ç–ª–µ–º–µ–Ω—Ç–∞
                            paragraphs = elem.find_all(['p', 'div'], recursive=True)
                            if paragraphs:
                                for p in paragraphs:
                                    text = p.get_text(strip=True)
                                    if text and len(text) > 20 and text not in seen_texts:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏
                                        content_parts.append(text)
                                        seen_texts.add(text)
                            else:
                                # –ï—Å–ª–∏ –Ω–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤, –±–µ—Ä–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —ç–ª–µ–º–µ–Ω—Ç–∞
                                text = elem.get_text(strip=True)
                                if text and len(text) > 50 and text not in seen_texts:
                                    content_parts.append(text)
                                    seen_texts.add(text)
                        if content_parts:
                            break
                
                # –ï—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –∏—â–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ —Å—Ç–∞—Ç—å–µ
                if not content_parts:
                    # –ò—â–µ–º –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ —Å—Ç–∞—Ç—å–∏
                    main_containers = soup.find_all(['article', 'main', '.content', '.post-content'])
                    for container in main_containers:
                        if container:
                            paragraphs = container.find_all('p')
                            for p in paragraphs:
                                text = p.get_text(strip=True)
                                if text and len(text) > 20 and text not in seen_texts:
                                    content_parts.append(text)
                                    seen_texts.add(text)
                            if content_parts:
                                break
                
                # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –∏—â–µ–º –ª—é–±—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                if not content_parts:
                    all_paragraphs = soup.find_all('p')
                    for p in all_paragraphs:
                        text = p.get_text(strip=True)
                        # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–∞–≤–∏–≥–∞—Ü–∏—é, —Ä–µ–∫–ª–∞–º—É –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã, –∞ —Ç–∞–∫–∂–µ –¥—É–±–ª–∏
                        if (text and len(text) > 30 and text not in seen_texts and
                            not any(skip_word in text.lower() for skip_word in 
                                   ['–Ω–∞–≤–∏–≥–∞—Ü–∏—è', '—Ä–µ–∫–ª–∞–º–∞', '–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è', '—Å–ª–µ–¥–∏—Ç—å', '–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π', '–ø–æ–¥–µ–ª–∏—Ç—å—Å—è'])):
                            content_parts.append(text)
                            seen_texts.add(text)
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏
                full_content = '\n\n'.join(content_parts)
                
                # –û—á–∏—â–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
                full_content = re.sub(r'[ \t]+', ' ', full_content)  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ —Ç–∞–±—ã
                full_content = re.sub(r'\n{3,}', '\n\n', full_content)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫
                full_content = full_content.strip()
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                if len(full_content) > 8000:
                    # –û–±—Ä–µ–∑–∞–µ–º –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É –ø–æ–ª–Ω–æ–º—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—é
                    truncated = full_content[:8000]
                    last_sentence = truncated.rfind('.')
                    if last_sentence > 6000:  # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ —Ä–∞–∑—É–º–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö
                        full_content = truncated[:last_sentence + 1] + "\n\n[–¢–µ–∫—Å—Ç —Å–æ–∫—Ä–∞—â–µ–Ω...]"
                    else:
                        full_content = truncated + "..."
                
                return full_content
                
        except Exception as e:
            logger.error(f"Error fetching full article {url}: {e}")
            return ""
    
    async def _extract_article_metadata(self, soup: BeautifulSoup) -> tuple[Optional[datetime], Optional[str], Optional[int], Optional[str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç—å–∏ –†–ò–ê: –¥–∞—Ç–∞, –≤—Ä–µ–º—è, –ø—Ä–æ—Å–º–æ—Ç—Ä—ã, –∞–≤—Ç–æ—Ä"""
        published_date = None
        published_time = None
        views_count = None
        author = None
        
        try:
            # –ò—â–µ–º –¥–∞—Ç—É –∏ –≤—Ä–µ–º—è –≤ –º–µ—Ç–∞-—Ç–µ–≥–∞—Ö
            date_meta = soup.find('meta', {'property': 'article:published_time'}) or \
                       soup.find('meta', {'name': 'mediator_published_time'})
            
            if date_meta:
                date_content = date_meta.get('content')
                if date_content:
                    try:
                        # –ü–∞—Ä—Å–∏–º ISO —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã
                        dt = datetime.fromisoformat(date_content.replace('Z', '+00:00'))
                        published_date = dt
                        published_time = dt.strftime('%H:%M')
                    except Exception as e:
                        logger.warning(f"Error parsing date: {e}")
                        pass
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –¥–∞—Ç—ã –≤ —Ç–µ–∫—Å—Ç–µ
            if not published_date:
                # –ò—â–µ–º –¥–∞—Ç—É –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –∏–ª–∏ –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–µ
                date_patterns = [
                    r'(\d{1,2}\s+\w+\s+\d{4})',  # "30 –∏—é–ª—è 2025"
                    r'(\d{1,2}\.\d{1,2}\.\d{4})',  # "30.07.2025"
                    r'(\d{4}-\d{1,2}-\d{1,2})'   # "2025-07-30"
                ]
                
                page_text = soup.get_text()
                for pattern in date_patterns:
                    match = re.search(pattern, page_text)
                    if match:
                        date_str = match.group(1)
                        parsed_date = self._parse_date(date_str)
                        if parsed_date:
                            published_date = parsed_date
                            break
            
            # –ò—â–µ–º –∞–≤—Ç–æ—Ä–∞
            author_selectors = [
                'meta[name="mediator_author"]',
                'meta[property="article:author"]',
                '.article__author',
                '.author-name',
                '[data-type="author"]'
            ]
            
            for selector in author_selectors:
                author_elem = soup.select_one(selector)
                if author_elem:
                    if author_elem.name == 'meta':
                        author = author_elem.get('content')
                    else:
                        author = author_elem.get_text(strip=True)
                    if author:
                        break
            
            # –ü—Ä–æ—Å–º–æ—Ç—Ä—ã –æ–±—ã—á–Ω–æ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –Ω–∞ –†–ò–ê, –ø–æ—ç—Ç–æ–º—É –æ—Å—Ç–∞–≤–ª—è–µ–º None
            
        except Exception as e:
            logger.warning(f"Error extracting RIA metadata: {e}")
        
        return published_date, published_time, views_count, author
    
    async def _fetch_full_article_with_metadata(self, url: str) -> tuple[str, Optional[datetime], Optional[str], Optional[int], Optional[str]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç–∞—Ç—å–∏ –≤–º–µ—Å—Ç–µ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        try:
            async with self.session.get(url) as response:
                if response.status != 200:
                    logger.warning(f"Failed to fetch article {url}: {response.status}")
                    return "", None, None, None, None
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                published_date, published_time, views_count, author = await self._extract_article_metadata(soup)
                
                # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe']):
                    element.decompose()
                
                # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏ –†–ò–ê
                content_parts = []
                
                # –û—Å–Ω–æ–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –†–ò–ê
                content_selectors = [
                    'div.article__body',
                    'div.article__text',
                    'div.article-text',
                    'div[data-type="text"]',
                    '.article__content p',
                    '.article-body p'
                ]
                
                for selector in content_selectors:
                    elements = soup.select(selector)
                    if elements:
                        for elem in elements:
                            text = elem.get_text(strip=True)
                            if text and len(text) > 30:  # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
                                content_parts.append(text)
                        if content_parts:
                            break
                
                # –ï—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –∏—â–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ —Å—Ç–∞—Ç—å–µ
                if not content_parts:
                    article_container = soup.find('article') or soup.find('main')
                    if article_container:
                        paragraphs = article_container.find_all('p')
                        for p in paragraphs:
                            text = p.get_text(strip=True)
                            if text and len(text) > 30:
                                content_parts.append(text)
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
                full_content = ' '.join(content_parts)
                
                # –û—á–∏—â–∞–µ–º –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                full_content = re.sub(r'\s+', ' ', full_content).strip()
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                if len(full_content) > 6000:
                    full_content = full_content[:6000] + "..."
                
                return full_content, published_date, published_time, views_count, author
                
        except Exception as e:
            logger.error(f"Error fetching full article {url}: {e}")
            return "", None, None, None, None
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏–∑ —Å—Ç—Ä–æ–∫–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä—É—Å—Å–∫–∏—Ö –º–µ—Å—è—Ü–µ–≤"""
        if not date_str:
            return None
            
        # –†—É—Å—Å–∫–∏–µ –º–µ—Å—è—Ü—ã –¥–ª—è –†–ò–ê
        russian_months = {
            '—è–Ω–≤–∞—Ä—è': '01', '—Ñ–µ–≤—Ä–∞–ª—è': '02', '–º–∞—Ä—Ç–∞': '03', '–∞–ø—Ä–µ–ª—è': '04',
            '–º–∞—è': '05', '–∏—é–Ω—è': '06', '–∏—é–ª—è': '07', '–∞–≤–≥—É—Å—Ç–∞': '08',
            '—Å–µ–Ω—Ç—è–±—Ä—è': '09', '–æ–∫—Ç—è–±—Ä—è': '10', '–Ω–æ—è–±—Ä—è': '11', '–¥–µ–∫–∞–±—Ä—è': '12',
            '—è–Ω–≤': '01', '—Ñ–µ–≤': '02', '–º–∞—Ä': '03', '–∞–ø—Ä': '04',
            '–º–∞–π': '05', '–∏—é–Ω': '06', '–∏—é–ª': '07', '–∞–≤–≥': '08',
            '—Å–µ–Ω': '09', '–æ–∫—Ç': '10', '–Ω–æ—è': '11', '–¥–µ–∫': '12'
        }
        
        # –ó–∞–º–µ–Ω—è–µ–º —Ä—É—Å—Å–∫–∏–µ –º–µ—Å—è—Ü—ã –Ω–∞ —á–∏—Å–ª–∞
        date_str_lower = date_str.lower()
        for ru_month, month_num in russian_months.items():
            if ru_month in date_str_lower:
                date_str = re.sub(ru_month, month_num, date_str_lower)
                break
        
        # –§–æ—Ä–º–∞—Ç—ã –¥–∞—Ç –¥–ª—è –†–ò–ê
        date_formats = [
            "%d %m %Y",      # "30 07 2025"
            "%d.%m.%Y",      # "30.07.2025"
            "%Y-%m-%d",      # "2025-07-30"
            "%d/%m/%Y"       # "30/07/2025"
        ]
        
        for date_format in date_formats:
            try:
                return datetime.strptime(date_str.strip(), date_format)
            except ValueError:
                continue
                
        logger.warning(f"Could not parse RIA date: {date_str}")
        return None
    
    async def parse_news_list_with_batch_save(self, max_articles: int = 10, date_filter: Optional[str] = None, 
                                            fetch_full_content: bool = True, batch_size: int = 10, 
                                            save_callback=None) -> List[NewsSource]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å–ø–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–∞–∫–µ—Ç–∞–º–∏"""
        try:
            news_items = []
            batch_buffer = []
            seen_urls = set()
            seen_titles = set()
            current_url = self.news_url
            page_num = 1
            total_saved = 0
            
            while len(news_items) < max_articles:
                logger.info(f"Parsing RIA page {page_num}, current articles: {len(news_items)}")
                
                # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
                async with self.session.get(current_url) as response:
                    if response.status != 200:
                        logger.error(f"Failed to fetch news list from {current_url}: {response.status}")
                        break
                    
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –†–ò–ê
                    news_containers = soup.find_all('div', class_='list-item__content')
                    
                    if not news_containers:
                        logger.info(f"No news containers found on page {page_num}")
                        break
                    
                    logger.info(f"Found {len(news_containers)} news containers on page {page_num}")
                    page_added = 0
                    
                    for container in news_containers:
                        if len(news_items) >= max_articles:
                            break
                            
                        try:
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç–∞—Ç—å—é
                            title_link = container.find('a', class_='list-item__title')
                            if not title_link:
                                continue
                            
                            article_url = title_link.get('href')
                            if not article_url:
                                continue
                            
                            # –î–µ–ª–∞–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π URL
                            full_url = urljoin(self.base_url, article_url)
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏ –ø–æ URL
                            if full_url in seen_urls:
                                continue
                            seen_urls.add(full_url)
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                            title = title_link.get_text(strip=True)
                            if not title:
                                continue
                                
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
                            if title in seen_titles:
                                continue
                            seen_titles.add(title)
                            
                            # –ï—Å–ª–∏ –Ω—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –ø–∞—Ä—Å–∏–º —Å—Ç–∞—Ç—å—é —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
                            full_content = ""
                            published_date = None
                            published_time = None
                            views_count = None
                            author = None
                            
                            if fetch_full_content:
                                full_content, published_date, published_time, views_count, author = await self._fetch_full_article_with_metadata(full_url)
                            
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∫–∞–∫ –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –µ—Å–ª–∏ –Ω–µ—Ç –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                            content = full_content if full_content else title
                            
                            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
                            news_item = NewsSource(
                                title=title,
                                url=full_url,
                                content=content,
                                published_date=published_date,
                                published_time=published_time,
                                views_count=views_count,
                                author=author,
                                source_site="ria.ru"
                            )
                            
                            if self._is_relevant_news(news_item, date_filter):
                                news_items.append(news_item)
                                batch_buffer.append(news_item)
                                page_added += 1
                                logger.info(f"Added news {len(news_items)}: {title[:50]}...")
                                
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–∞–∫–µ—Ç
                                if len(batch_buffer) >= batch_size and save_callback:
                                    logger.info(f"üîÑ –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞–∫–µ—Ç –∏–∑ {len(batch_buffer)} —Å—Ç–∞—Ç–µ–π...")
                                    saved_count = await save_callback(batch_buffer.copy())
                                    total_saved += saved_count
                                    logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count} —Å—Ç–∞—Ç–µ–π. –í—Å–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {total_saved}")
                                    batch_buffer.clear()
                                
                                # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                                await asyncio.sleep(0.5)
                                
                        except Exception as e:
                            logger.warning(f"Error parsing article container: {e}")
                            continue
                    
                    logger.info(f"Added {page_added} articles from page {page_num}")
                    
                    # –ï—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏–º–∏—Ç–∞, –≤—ã—Ö–æ–¥–∏–º
                    if len(news_items) >= max_articles:
                        break
                    
                    # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É (–∫–Ω–æ–ø–∫–∞ "–ï—â–µ" –∏–ª–∏ data-next-url)
                    data_url = None
                    
                    # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –∫–Ω–æ–ø–∫—É "–ï—â–µ"
                    more_button = soup.find('div', class_='list-more')
                    if more_button:
                        data_url = more_button.get('data-url')
                        logger.info("Found 'more' button with data-url")
                    
                    # –ï—Å–ª–∏ –∫–Ω–æ–ø–∫–∏ –Ω–µ—Ç, –∏—â–µ–º data-next-url –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ
                    if not data_url:
                        next_container = soup.find('div', {'data-next-url': True})
                        if next_container:
                            data_url = next_container.get('data-next-url')
                            logger.info("Found data-next-url in container")
                    
                    # –ï—Å–ª–∏ –Ω–µ—Ç data-url, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –ø–∞–≥–∏–Ω–∞—Ü–∏—é
                    if not data_url:
                        pagination_links = soup.find_all('a', class_='pagination__item')
                        for link in pagination_links:
                            if '–î–∞–ª–µ–µ' in link.get_text() or 'next' in link.get('class', []):
                                data_url = link.get('href')
                                break
                    
                    if data_url:
                        # –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                        if data_url.startswith('/'):
                            current_url = urljoin(self.base_url, data_url)
                        elif data_url.startswith('http'):
                            current_url = data_url
                        else:
                            current_url = urljoin(current_url, data_url)
                        
                        logger.info(f"Moving to next page: {current_url}")
                        page_num += 1
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
                        await asyncio.sleep(1)
                    else:
                        logger.info("No more pages found")
                        break
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Å—Ç–∞—Ç—å–∏ –≤ –±—É—Ñ–µ—Ä–µ
            if batch_buffer and save_callback:
                logger.info(f"üîÑ –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–∞–∫–µ—Ç –∏–∑ {len(batch_buffer)} —Å—Ç–∞—Ç–µ–π...")
                saved_count = await save_callback(batch_buffer.copy())
                total_saved += saved_count
                logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count} —Å—Ç–∞—Ç–µ–π. –ò—Ç–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {total_saved}")
            
            logger.info(f"Parsing completed. Total articles: {len(news_items)}, Total saved: {total_saved}")
            return news_items
            
        except Exception as e:
            logger.error(f"Error in parse_news_list_with_batch_save: {e}")
            return []