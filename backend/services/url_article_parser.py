"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç–µ–π –∏–∑ –ª—é–±—ã—Ö URL —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Jina AI Reader API
"""

import asyncio
import aiohttp
import logging
from typing import Optional, Dict, Any
from urllib.parse import urlparse
from pydantic import HttpUrl

logger = logging.getLogger(__name__)


class URLArticleParser:
    """
    –ü–∞—Ä—Å–µ—Ä —Å—Ç–∞—Ç–µ–π –∏–∑ –ª—é–±—ã—Ö URL —á–µ—Ä–µ–∑ Jina AI Reader API

    Jina AI Reader API - –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü –≤ markdown
    –õ–∏–º–∏—Ç: 1M –∑–∞–ø—Ä–æ—Å–æ–≤/–º–µ—Å—è—Ü –±–µ—Å–ø–ª–∞—Ç–Ω–æ
    –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: https://jina.ai/reader
    """

    JINA_READER_BASE_URL = "https://r.jina.ai"
    REQUEST_TIMEOUT = 30  # —Å–µ–∫—É–Ω–¥

    def __init__(self):
        self.session: Optional[aiohttp.ClientSession] = None

    async def __aenter__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è HTTP —Å–µ—Å—Å–∏–∏"""
        import ssl

        # –°–æ–∑–¥–∞–µ–º SSL –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –æ—Ç–∫–ª—é—á–µ–Ω–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π –¥–ª—è development
        # –í production —ç—Ç–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤–∫–ª—é—á–µ–Ω–æ!
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE

        connector = aiohttp.TCPConnector(ssl=ssl_context)

        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=aiohttp.ClientTimeout(total=self.REQUEST_TIMEOUT),
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ HTTP —Å–µ—Å—Å–∏–∏"""
        if self.session:
            await self.session.close()

    def _validate_url(self, url: str) -> bool:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è URL

        Args:
            url: URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏

        Returns:
            True –µ—Å–ª–∏ URL –≤–∞–ª–∏–¥–µ–Ω, –∏–Ω–∞—á–µ False
        """
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except Exception as e:
            logger.error(f"URL validation error: {e}")
            return False

    def _extract_domain(self, url: str) -> str:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–º–µ–Ω–∞ –∏–∑ URL

        Args:
            url: URL

        Returns:
            –î–æ–º–µ–Ω–Ω–æ–µ –∏–º—è
        """
        try:
            parsed = urlparse(url)
            return parsed.netloc
        except Exception:
            return "unknown"

    def _clean_content(self, content: str) -> str:
        """
        –û—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –æ—Ç –Ω–∞–≤–∏–≥–∞—Ü–∏–∏, —Ä–µ–∫–ª–∞–º—ã –∏ –ª–∏—à–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤

        Args:
            content: –ò—Å—Ö–æ–¥–Ω—ã–π markdown –∫–æ–Ω—Ç–µ–Ω—Ç

        Returns:
            –û—á–∏—â–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        """
        import re

        # –£–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–æ—Ü—Å–µ—Ç–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ [text](url)
        content = re.sub(r'\[.*?\]\(https?://(vk\.com|twitter\.com|facebook\.com|t\.me|instagram\.com|youtube\.com|ok\.ru|zen\.yandex\.ru)/[^\)]*\)', '', content)

        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Ç–æ–ª—å–∫–æ —Å —Å–∏–º–≤–æ–ª–∞–º–∏/—ç–º–æ–¥–∑–∏/–∏–∫–æ–Ω–∫–∞–º–∏
        content = re.sub(r'^[\s\*\[\]()‚Ä¢‚ñ™‚ñ´‚ó¶‚Ä£‚ÅÉ]+$', '', content, flags=re.MULTILINE)

        # –£–¥–∞–ª—è–µ–º copyright –∏ –≥–æ–¥—ã
        content = re.sub(r'¬©\s*\d{4}(-\d{4})?', '', content)

        # –£–¥–∞–ª—è–µ–º email –∞–¥—Ä–µ—Å–∞
        content = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '', content)

        # –£–¥–∞–ª—è–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ +7, 8-800 –∏ —Ç.–¥.
        content = re.sub(r'[\+]?[(]?[0-9]{1,3}[)]?[-\s\.]?[(]?[0-9]{1,4}[)]?[-\s\.]?[0-9]{1,4}[-\s\.]?[0-9]{1,9}', '', content)

        # –£–¥–∞–ª—è–µ–º –ò–ù–ù, –û–ì–†–ù, –û–ö–ü–û (—Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã)
        content = re.sub(r'\b(–ò–ù–ù|–û–ì–†–ù|–û–ö–ü–û|–ö–ü–ü|–û–ö–í–≠–î)[\s:]*\d+', '', content, flags=re.IGNORECASE)

        # –£–¥–∞–ª—è–µ–º –∞–¥—Ä–µ—Å–∞ (–≥–æ—Ä–æ–¥, —É–ª–∏—Ü–∞, –¥–æ–º)
        content = re.sub(r'\b(–≥\.|–≥–æ—Ä–æ–¥|—É–ª\.|—É–ª–∏—Ü–∞|–ø—Ä\.|–ø—Ä–æ—Å–ø–µ–∫—Ç|–¥\.|–¥–æ–º)\s+[–ê-–Ø–∞-—è—ë–Å0-9\s,.-]+', '', content)

        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π/–ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤
        content = re.sub(r'\*{2,}\s*(–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö|–ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤|–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π)', '', content, flags=re.IGNORECASE)

        lines = content.split('\n')
        cleaned_lines = []
        skip_until_title = True

        for line in lines:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—Å—ë –¥–æ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ Title: –∏–ª–∏ –ø–µ—Ä–≤–æ–≥–æ H1
            if skip_until_title:
                if line.startswith('Title:') or line.startswith('# '):
                    skip_until_title = False
                    cleaned_lines.append(line)
                continue

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –Ω–∞–≤–∏–≥–∞—Ü–∏–µ–π, —Ä–µ–∫–ª–∞–º–æ–π, –º–æ–¥–∞–ª—å–Ω—ã–º–∏ –æ–∫–Ω–∞–º–∏ –∏ footer
            if any(keyword in line.lower() for keyword in [
                # –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è
                'image ', '–≤–æ–π—Ç–∏', '–≤—Ö–æ–¥', '–∞–≤—Ç–æ—Ä–∏–∑—É–π—Ç–µ—Å—å', '–∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞—Ç—å—Å—è', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è', '–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è',
                '–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏', '–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å', '–∑–∞–≥—Ä—É–∑–∏—Ç—å –µ—â–µ',
                # –ü–æ–¥–ø–∏—Å–∫–∏ –∏ —Ä–µ–∫–ª–∞–º–∞
                '–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è', '–ø–æ–¥–ø–∏—Å–∫–∞', '—Ä–µ–∫–ª–∞–º–∞', 'banner', '–Ω–∞–ø–∏—Å–∞—Ç—å –Ω–∞–º',
                # Cookie –∏ –º–æ–¥–∞–ª—å–Ω—ã–µ –æ–∫–Ω–∞
                'cookie', '–ø—Ä–∏–Ω—è—Ç—å', '–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ', '—Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–ª–µ—Ç–Ω', '–≤–æ–∑—Ä–∞—Å—Ç', '18+',
                # –°–æ—Ü—Å–µ—Ç–∏ –∏ –º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä—ã
                '[](http', 'vk.com', 'telegram', '–ø–æ–¥–µ–ª–∏—Ç—å—Å—è', 'twitter', 'whatsapp', 'viber', 'skype',
                'facebook', 'youtube', 'instagram', '—è–Ω–¥–µ–∫—Å.–º–µ—Ç—Ä–∏–∫–∞',
                # Footer –∏ –∫–æ–Ω—Ç–∞–∫—Ç—ã
                '—Ä–µ–∫–ª–∞–º–æ–¥–∞—Ç–µ–ª—è–º', '–∫–æ–Ω—Ç–∞–∫—Ç—ã', '—Ä–µ–¥–∞–∫—Ü–∏—è', '–ø–æ–ª–∏—Ç–∏–∫–∞', '–æ—Ñ–µ—Ä—Ç–∞', '–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ —Å–æ–≥–ª–∞—à–µ–Ω–∏–µ',
                '–∏–Ω–Ω', '–æ–≥—Ä–Ω', '–æ–∫–ø–æ', '—é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞–¥—Ä–µ—Å', '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏', '–æ–æ–æ', '–∞–æ', '–∑–∞–æ',
                # –ù–∞–≤–∏–≥–∞—Ü–∏—è
                '—á–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–∂–µ', '–æ –Ω–∞—Å', '–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è', '—ç–∫—Å–ø–µ—Ä—Ç—ã', '—Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏', '–≥–ª–∞–≤–Ω–∞—è',
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
                '–≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤', '–∏—Å—Ç–æ—á–Ω–∏–∫:', '—Ñ–æ—Ç–æ:', '–≤–∏–¥–µ–æ:',
                '–≤—Å–µ –ø—Ä–∞–≤–∞ –∑–∞—â–∏—â–µ–Ω—ã', '–ø–µ—Ä–µ–ø–µ—á–∞—Ç–∫–∞', '–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤',
                '–Ω–æ–≤–æ—Å—Ç–Ω–∞—è –ª–µ–Ω—Ç–∞', '–ø–æ–ø—É–ª—è—Ä–Ω–æ–µ', '–ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏', '—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º', '–ø–æ —Ç–µ–º–µ',
                '—Å–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ', '–±–æ–ª—å—à–µ –Ω–æ–≤–æ—Å—Ç–µ–π', '—Å–ª–µ–¥–∏—Ç–µ –∑–∞ –Ω–∞–º–∏', '–±–ª–∏–∂–∞–π—à–∏–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è',
                '—É—Å–ª–æ–≤–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è', '—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ', '–ø–æ–¥—Ä–æ–±–Ω–µ–µ'
            ]):
                continue

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –ø–æ–¥—Ä—è–¥ (–±–æ–ª—å—à–µ 2)
            if not line.strip():
                if cleaned_lines and not cleaned_lines[-1].strip():
                    continue

            cleaned_lines.append(line)

        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤ –∫–æ–Ω—Ü–µ
        while cleaned_lines and not cleaned_lines[-1].strip():
            cleaned_lines.pop()

        return '\n'.join(cleaned_lines)

    def _extract_metadata(self, content: str) -> Dict[str, Optional[str]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ markdown –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ –æ—Ç Jina AI Reader

        Args:
            content: Markdown –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç Jina

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏: title, description, published_date, author
        """
        import re
        from datetime import datetime

        metadata = {
            'title': None,
            'description': None,
            'published_date': None,
            'author': None
        }

        lines = content.split('\n')

        for i, line in enumerate(lines):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º Title (–æ–±—ã—á–Ω–æ –≤ –Ω–∞—á–∞–ª–µ –æ—Ç Jina)
            if line.startswith('Title:') and not metadata['title']:
                metadata['title'] = line.replace('Title:', '').strip()

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–≤—ã–π H1 –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–æ–∫, –µ—Å–ª–∏ Title –Ω–µ –Ω–∞–π–¥–µ–Ω
            elif line.startswith('# ') and not metadata['title']:
                metadata['title'] = line.replace('# ', '').strip()

            # –ò–∑–≤–ª–µ–∫–∞–µ–º URL Source (–æ–±—ã—á–Ω–æ —Å–ª–µ–¥—É–µ—Ç –∑–∞ Title)
            elif line.startswith('URL Source:'):
                pass  # –£–∂–µ –µ—Å—Ç—å –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –º–µ—Ç–æ–¥–µ

            # –ò–∑–≤–ª–µ–∫–∞–µ–º Published Time
            elif line.startswith('Published Time:'):
                date_str = line.replace('Published Time:', '').strip()
                try:
                    # –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ ISO —Ñ–æ—Ä–º–∞—Ç–∞
                    metadata['published_date'] = datetime.fromisoformat(date_str.replace('Z', '+00:00')).isoformat()
                except:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –µ—Å—Ç—å, –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å
                    metadata['published_date'] = date_str

            # –ò–∑–≤–ª–µ–∫–∞–µ–º Author (–µ—Å–ª–∏ –µ—Å—Ç—å)
            elif line.startswith('Author:'):
                metadata['author'] = line.replace('Author:', '').strip()

            # –ò—â–µ–º –¥–∞—Ç—É –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç–∞—Ç—å–∏ (—Ñ–æ—Ä–º–∞—Ç—ã: 06.10.2025, 2025-10-06)
            elif not metadata['published_date'] and i < 20:  # –ò—â–µ–º –≤ –ø–µ—Ä–≤—ã—Ö 20 —Å—Ç—Ä–æ–∫–∞—Ö
                # –§–æ—Ä–º–∞—Ç DD.MM.YYYY –∏–ª–∏ DD-MM-YYYY
                date_match = re.search(r'\b(\d{2})[.\-/](\d{2})[.\-/](\d{4})\b', line)
                if date_match:
                    try:
                        day, month, year = date_match.groups()
                        metadata['published_date'] = f"{year}-{month}-{day}"
                    except:
                        pass

                # –§–æ—Ä–º–∞—Ç YYYY-MM-DD
                date_match = re.search(r'\b(\d{4})[.\-/](\d{2})[.\-/](\d{2})\b', line)
                if date_match and not metadata['published_date']:
                    metadata['published_date'] = date_match.group(0)

        # –ï—Å–ª–∏ description –Ω–µ –Ω–∞–π–¥–µ–Ω, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        if not metadata['description']:
            found_title = False
            for line in lines:
                if line.startswith('Title:') or line.startswith('# '):
                    found_title = True
                    continue

                if found_title and line.strip() and not line.startswith('#') and not line.startswith('URL'):
                    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏–ª–∏ –ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤
                    desc = line.strip()
                    if len(desc) > 200:
                        desc = desc[:200] + '...'
                    metadata['description'] = desc
                    break

        return metadata

    async def _parse_via_trafilatura(self, url: str) -> Optional[str]:
        """
        Fallback –ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ trafilatura –¥–ª—è —Å–ª—É—á–∞–µ–≤, –∫–æ–≥–¥–∞ Jina –Ω–µ —Å–ø—Ä–∞–≤–∏–ª—Å—è

        Args:
            url: URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞

        Returns:
            –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            from trafilatura import fetch_url, extract

            logger.info(f"üîß [TRAFILATURA] ========== START FALLBACK ==========")
            logger.info(f"üîß [TRAFILATURA] URL: {url}")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            logger.info(f"üîß [TRAFILATURA] Downloading page...")
            downloaded = fetch_url(url)
            if not downloaded:
                logger.warning(f"üîß [TRAFILATURA] ‚ùå Failed to download URL: {url}")
                return None
            
            logger.info(f"üîß [TRAFILATURA] ‚úÖ Downloaded {len(downloaded)} bytes")

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
            logger.info(f"üîß [TRAFILATURA] Extracting content...")
            text = extract(
                downloaded,
                include_comments=False,  # –ë–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
                include_tables=True,     # –° —Ç–∞–±–ª–∏—Ü–∞–º–∏ (–º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã –≤ —Å—Ç–∞—Ç—å—è—Ö)
                no_fallback=False,       # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å fallback –º–µ—Ç–æ–¥—ã
                favor_precision=True,    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞–¥ –ø–æ–ª–Ω–æ—Ç–æ–π
                deduplicate=True,        # –£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã
            )

            if text and len(text) > 100:
                logger.info(f"üîß [TRAFILATURA] ‚úÖ Extracted {len(text)} characters")
                logger.info(f"üîß [TRAFILATURA] Preview: {text[:200]}...")
                return text
            else:
                logger.warning(f"üîß [TRAFILATURA] ‚ùå Content too short: {len(text) if text else 0} chars")
                return None

        except ImportError:
            logger.error("üîß [TRAFILATURA] ‚ùå Not installed! Run: pip install trafilatura")
            return None
        except Exception as e:
            logger.error(f"üîß [TRAFILATURA] ‚ùå Error: {str(e)}", exc_info=True)
            return None

    async def parse_article(self, url: str) -> Dict[str, Any]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç—å–∏ –∏–∑ URL —á–µ—Ä–µ–∑ Jina AI Reader

        Args:
            url: URL —Å—Ç–∞—Ç—å–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ —Å—Ç–∞—Ç—å–∏:
            {
                'success': bool,
                'url': str,
                'content': str,  # –¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –≤ markdown
                'domain': str,
                'title': Optional[str],  # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏
                'description': Optional[str],  # –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
                'published_date': Optional[str],  # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                'author': Optional[str],  # –ê–≤—Ç–æ—Ä
                'error': Optional[str]
            }
        """
        if not self._validate_url(url):
            return {
                'success': False,
                'url': url,
                'content': '',
                'domain': '',
                'error': 'Invalid URL format'
            }

        domain = self._extract_domain(url)

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Jina AI Reader:
        # - X-Return-Format: markdown (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        # - X-With-Links-Summary: false (–±–µ–∑ —Å–ø–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫ –≤ –∫–æ–Ω—Ü–µ)
        # - X-With-Images-Summary: false (–±–µ–∑ —Å–ø–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
        # - X-No-Cache: false (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–µ—à)
        jina_url = f"{self.JINA_READER_BASE_URL}/{url}"

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'X-With-Links-Summary': 'false',
            'X-With-Images-Summary': 'false',
            # –¶–µ–ª–µ–≤—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            'X-Target-Selector': 'article,main,.post-content,.article-body,.entry-content,.content,.post,.article-text',
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è (–º–æ–¥–∞–ª—å–Ω—ã–µ –æ–∫–Ω–∞, footer, –Ω–∞–≤–∏–≥–∞—Ü–∏—è, —Ä–µ–∫–ª–∞–º–∞)
            'X-Remove-Selector': 'nav,header,footer,aside,.sidebar,.advertisement,.social-share,#comments,.cookie-notice,.related-posts,.newsletter,.popup,.promo,.banner,.widget,.ad,#sidebar,.share-buttons,.author-box,.modal,.age-verification,.login-prompt,.auth-form,.registration-form,.gdpr-notice,.contact-info,.company-info,.legal-info,.breadcrumbs,.tags,.categories,.popular-posts,.recent-posts,.upcoming-events,.event-list,.social-links,.footer-menu,.header-menu,.site-nav,.site-footer,.site-header',
        }

        logger.info(f"üåê [JINA] ========== START PARSING ==========")
        logger.info(f"üåê [JINA] Target URL: {url}")
        logger.info(f"üåê [JINA] Jina Reader URL: {jina_url}")
        logger.info(f"üåê [JINA] Headers: {headers}")
        logger.info(f"üåê [JINA] Timeout: {self.REQUEST_TIMEOUT}s")

        try:
            if not self.session:
                raise RuntimeError("Session not initialized. Use 'async with' context manager.")

            logger.info(f"üåê [JINA] Sending GET request...")
            async with self.session.get(jina_url, headers=headers) as response:
                logger.info(f"üåê [JINA] Response status: {response.status}")
                logger.info(f"üåê [JINA] Response headers: {dict(response.headers)}")
                
                if response.status == 200:
                    raw_content = await response.text()

                    # –û—á–∏—â–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç –º—É—Å–æ—Ä–∞
                    content = self._clean_content(raw_content)

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –ø—É—Å—Ç–æ–π
                    if not content or len(content.strip()) < 100:
                        logger.warning(f"Content too short from Jina, trying trafilatura fallback for URL: {url}")

                        # –ü—Ä–æ–±—É–µ–º trafilatura –∫–∞–∫ –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç
                        fallback_content = await self._parse_via_trafilatura(url)

                        if fallback_content and len(fallback_content) > len(content):
                            logger.info(f"Trafilatura fallback successful for {url}")
                            # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                            metadata = self._extract_metadata(raw_content)
                            return {
                                'success': True,
                                'url': url,
                                'content': fallback_content,
                                'domain': domain,
                                'title': metadata['title'],
                                'description': metadata['description'],
                                'published_date': metadata['published_date'],
                                'author': metadata['author'],
                                'error': None
                            }

                        return {
                            'success': False,
                            'url': url,
                            'content': content,
                            'domain': domain,
                            'title': None,
                            'description': None,
                            'published_date': None,
                            'author': None,
                            'error': 'Content too short or empty'
                        }

                    logger.info(f"Successfully parsed article from {url} (raw: {len(raw_content)} chars, cleaned: {len(content)} chars)")

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    metadata = self._extract_metadata(raw_content)

                    return {
                        'success': True,
                        'url': url,
                        'content': content,
                        'domain': domain,
                        'title': metadata['title'],
                        'description': metadata['description'],
                        'published_date': metadata['published_date'],
                        'author': metadata['author'],
                        'error': None
                    }
                else:
                    # –ß–∏—Ç–∞–µ–º —Ç–µ–ª–æ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                    error_body = await response.text()
                    error_msg = f"HTTP {response.status}: {response.reason}"
                    logger.error(f"‚ùå [JINA] Failed to parse {url}")
                    logger.error(f"‚ùå [JINA] Status: {response.status}")
                    logger.error(f"‚ùå [JINA] Reason: {response.reason}")
                    logger.error(f"‚ùå [JINA] Response body: {error_body[:500] if error_body else 'empty'}")
                    logger.info(f"üîÑ [JINA] Trying trafilatura fallback...")
                    
                    # –ü—Ä–æ–±—É–µ–º trafilatura –ø—Ä–∏ HTTP –æ—à–∏–±–∫–∞—Ö –æ—Ç Jina
                    fallback_content = await self._parse_via_trafilatura(url)
                    if fallback_content:
                        logger.info(f"‚úÖ [JINA] Trafilatura fallback successful! Got {len(fallback_content)} chars")
                        return {
                            'success': True,
                            'url': url,
                            'content': fallback_content,
                            'domain': domain,
                            'title': None,
                            'description': None,
                            'published_date': None,
                            'author': None,
                            'error': None
                        }
                    
                    logger.error(f"‚ùå [JINA] Trafilatura fallback also failed")
                    return {
                        'success': False,
                        'url': url,
                        'content': '',
                        'domain': domain,
                        'title': None,
                        'description': None,
                        'published_date': None,
                        'author': None,
                        'error': error_msg
                    }

        except asyncio.TimeoutError:
            logger.error(f"‚è±Ô∏è [JINA] TIMEOUT after {self.REQUEST_TIMEOUT}s for {url}")
            logger.info(f"üîÑ [JINA] Trying trafilatura fallback after timeout...")
            
            # –ü—Ä–æ–±—É–µ–º trafilatura –ø—Ä–∏ —Ç–∞–π–º–∞—É—Ç–µ Jina
            fallback_content = await self._parse_via_trafilatura(url)
            if fallback_content:
                logger.info(f"‚úÖ [JINA] Trafilatura fallback successful after timeout!")
                return {
                    'success': True,
                    'url': url,
                    'content': fallback_content,
                    'domain': domain,
                    'title': None,
                    'description': None,
                    'published_date': None,
                    'author': None,
                    'error': None
                }

            logger.error(f"‚ùå [JINA] Trafilatura also failed after Jina timeout")
            return {
                'success': False,
                'url': url,
                'content': '',
                'domain': domain,
                'title': None,
                'description': None,
                'published_date': None,
                'author': None,
                'error': f"Jina timeout after {self.REQUEST_TIMEOUT}s, trafilatura also failed"
            }

        except aiohttp.ClientError as e:
            error_msg = f"Network error: {str(e)}"
            logger.error(f"üåê [JINA] Network error: {error_msg}")
            logger.info(f"üîÑ [JINA] Trying trafilatura fallback...")

            # –ü—Ä–æ–±—É–µ–º trafilatura –ø—Ä–∏ —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–∫–∞—Ö Jina
            fallback_content = await self._parse_via_trafilatura(url)
            if fallback_content:
                logger.info(f"‚úÖ [JINA] Trafilatura fallback successful!")
                return {
                    'success': True,
                    'url': url,
                    'content': fallback_content,
                    'domain': domain,
                    'title': None,
                    'description': None,
                    'published_date': None,
                    'author': None,
                    'error': None
                }

            logger.error(f"‚ùå [JINA] Trafilatura also failed")
            return {
                'success': False,
                'url': url,
                'content': '',
                'domain': domain,
                'title': None,
                'description': None,
                'published_date': None,
                'author': None,
                'error': error_msg
            }
        except Exception as e:
            error_msg = f"Unexpected error: {str(e)}"
            logger.error(f"‚ùå [JINA] Unexpected error: {error_msg}", exc_info=True)
            logger.info(f"üîÑ [JINA] Trying trafilatura fallback...")
            
            # –ü—Ä–æ–±—É–µ–º trafilatura –ø—Ä–∏ –ª—é–±—ã—Ö –æ—à–∏–±–∫–∞—Ö
            fallback_content = await self._parse_via_trafilatura(url)
            if fallback_content:
                logger.info(f"‚úÖ [JINA] Trafilatura fallback successful!")
                return {
                    'success': True,
                    'url': url,
                    'content': fallback_content,
                    'domain': domain,
                    'title': None,
                    'description': None,
                    'published_date': None,
                    'author': None,
                    'error': None
                }
            
            return {
                'success': False,
                'url': url,
                'content': '',
                'domain': domain,
                'title': None,
                'description': None,
                'published_date': None,
                'author': None,
                'error': error_msg
            }

    async def parse_multiple_urls(self, urls: list[str]) -> list[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö URL –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ

        Args:
            urls: –°–ø–∏—Å–æ–∫ URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–∞—Ä—Å–∏–Ω–≥–∞
        """
        import asyncio
        tasks = [self.parse_article(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    'success': False,
                    'url': urls[i],
                    'content': '',
                    'domain': self._extract_domain(urls[i]),
                    'error': str(result)
                })
            else:
                processed_results.append(result)

        return processed_results


# Singleton instance –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
url_parser = URLArticleParser()
