"""
API endpoints –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç–∞—Ç–µ–π –∏–∑ –≤–Ω–µ—à–Ω–∏—Ö URL
"""

from fastapi import APIRouter, HTTPException, Depends, status
from sqlmodel import Session
from typing import Optional
import logging
import re

from database.models import (
    NewsGenerationDraft, User, ProjectType,
    GenerationLog, Article, SourceType, NewsStatus
)
from database.schemas import (
    URLArticleParseRequest, URLArticleParseResponse,
    URLArticleGenerationRequest, URLArticleGenerationResponse
)
from database.connection import get_session
from api.dependencies import require_staff
from services.url_article_parser import url_parser
from services.ai_service import get_ai_service
import json

logger = logging.getLogger(__name__)

router = APIRouter(tags=["URL Articles"])


@router.post("/parse", response_model=URLArticleParseResponse)
async def parse_url_article(
    request: URLArticleParseRequest,
    session: Session = Depends(get_session),
    current_user: User = Depends(require_staff)
):
    """
    –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç—å–∏ –∏–∑ URL (–±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Jina AI Reader –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –ë–î
    """
    try:
        logger.info(f"User {current_user.username} parsing URL: {request.url}")

        # –®–∞–≥ 1: –ü–∞—Ä—Å–∏–º URL —á–µ—Ä–µ–∑ Jina AI + trafilatura fallback
        async with url_parser:
            parse_result = await url_parser.parse_article(request.url)

        if not parse_result['success']:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=parse_result['error'] or "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å URL"
            )

        # –®–∞–≥ 2: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —á–µ—Ä–µ–∑ GPT-4o mini
        ai_service = get_ai_service()
        cleaned_content, cleaning_metrics = await ai_service.clean_article_content(
            parse_result['content'],
            request.url
        )

        # üîç –ü–†–û–í–ï–†–ö–ê FALLBACK: –µ—Å–ª–∏ GPT —É–ø–∞–ª - –ª–æ–≥–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ
        if cleaning_metrics.get('fallback', False):
            logger.error(f"üî¥ GPT FALLBACK! Returning raw content. Error: {cleaning_metrics.get('error', 'Unknown')}")
            logger.error(f"üî¥ Error type: {cleaning_metrics.get('error_type', 'Unknown')}, Attempts: {cleaning_metrics.get('attempts', 'Unknown')}")
        else:
            logger.info(f"‚úÖ Article cleaned: {cleaning_metrics.get('reduction_percent', 0)}% reduction, "
                       f"{cleaning_metrics.get('tokens_used', 0)} tokens, attempt {cleaning_metrics.get('attempt', 1)}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é
            if not cleaning_metrics.get('validation_passed', True):
                logger.warning(f"‚ö†Ô∏è Validation warnings: {', '.join(cleaning_metrics.get('validation_warnings', []))}")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        parse_result['content'] = cleaned_content

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–ø–µ—Ä–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ H1)
        content = parse_result['content']
        title = parse_result['domain']  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ–º–µ–Ω

        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ markdown
        h1_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        title_match = re.search(r'^Title:\s*(.+)$', content, re.MULTILINE)

        if h1_match:
            title = h1_match.group(1).strip()
        elif title_match:
            title = title_match.group(1).strip()
        else:
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É —Å —Ç–µ–∫—Å—Ç–æ–º
            first_line = next((line.strip() for line in content.split('\n') if line.strip() and len(line.strip()) > 10), None)
            if first_line:
                title = first_line[:200]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∑–∞–≥–æ–ª–æ–≤–∫–∞

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Å—Ç–∞—Ç—å—è —Å —Ç–∞–∫–∏–º URL
        existing_article = session.query(Article).filter(Article.url == parse_result['url']).first()

        if existing_article:
            logger.info(f"Article already exists: {existing_article.id}")
            article_id = existing_article.id
        else:
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å—Ç–∞—Ç—å—é –≤ –ë–î
            article = Article(
                title=title,
                url=parse_result['url'],
                content=content,
                source_site=SourceType.URL,
                published_date=None,
                author=parse_result['domain'],
                is_processed=False
            )

            session.add(article)
            session.commit()
            session.refresh(article)

            article_id = article.id
            logger.info(f"Created new article from URL: id={article_id}, title={title[:50]}")

        return URLArticleParseResponse(
            success=True,
            url=parse_result['url'],
            content=parse_result['content'],
            domain=parse_result['domain'],
            article_id=article_id,
            title=title,
            error=None
        )

    except HTTPException:
        raise
    except Exception as e:
        session.rollback()
        logger.error(f"Error parsing URL {request.url}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ URL: {str(e)}"
        )


@router.post("/generate-from-url", response_model=URLArticleGenerationResponse)
async def generate_article_from_url(
    request: URLArticleGenerationRequest,
    session: Session = Depends(get_session),
    current_user: User = Depends(require_staff)
):
    """
    –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª: –ø–∞—Ä—Å–∏–Ω–≥ URL + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Å—Ç–∞—Ç—å–∏

    1. –ü–∞—Ä—Å–∏—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —á–µ—Ä–µ–∑ Jina AI Reader
    2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é –º–µ–¥–∏—Ü–∏–Ω—Å–∫—É—é —Å—Ç–∞—Ç—å—é —á–µ—Ä–µ–∑ GPT
    3. –°–æ–∑–¥–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ YandexART (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    4. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–∞–∫ —á–µ—Ä–Ω–æ–≤–∏–∫ –≤ –ë–î
    """
    try:
        logger.info(f"User {current_user.username} generating article from URL: {request.url}")

        # –®–∞–≥ 1: –ü–∞—Ä—Å–∏–º URL
        async with url_parser:
            parse_result = await url_parser.parse_article(request.url)

        if not parse_result['success']:
            return URLArticleGenerationResponse(
                success=False,
                draft_id=None,
                source_url=request.url,
                source_domain=parse_result.get('domain', ''),
                news_text=None,
                seo_title=None,
                seo_description=None,
                seo_keywords=None,
                image_url=None,
                error=parse_result.get('error', '–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å URL')
            )

        source_url = parse_result['url']
        source_domain = parse_result['domain']
        raw_content = parse_result['content']

        logger.info(f"Successfully parsed {source_url}, raw content length: {len(raw_content)} chars")

        # –®–∞–≥ 1.5: –û—á–∏—Å—Ç–∫–∞ —á–µ—Ä–µ–∑ GPT-4o mini
        ai_service = get_ai_service()
        cleaned_content, cleaning_metrics = await ai_service.clean_article_content(
            raw_content,
            source_url
        )

        # üîç –ü–†–û–í–ï–†–ö–ê FALLBACK: –µ—Å–ª–∏ GPT —É–ø–∞–ª - –ª–æ–≥–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ
        if cleaning_metrics.get('fallback', False):
            logger.error(f"üî¥ GPT FALLBACK! Returning raw content. Error: {cleaning_metrics.get('error', 'Unknown')}")
            logger.error(f"üî¥ Error type: {cleaning_metrics.get('error_type', 'Unknown')}, Attempts: {cleaning_metrics.get('attempts', 'Unknown')}")
        else:
            logger.info(f"‚úÖ Article cleaned: {cleaning_metrics.get('reduction_percent', 0)}% reduction, "
                       f"{cleaning_metrics.get('tokens_used', 0)} tokens, attempt {cleaning_metrics.get('attempt', 1)}, "
                       f"from {len(raw_content)} to {len(cleaned_content)} chars")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é
            if not cleaning_metrics.get('validation_passed', True):
                logger.warning(f"‚ö†Ô∏è Validation warnings: {', '.join(cleaning_metrics.get('validation_warnings', []))}")

        external_content = cleaned_content

        # –®–∞–≥ 1.7: –°–æ–∑–¥–∞–µ–º –∏–ª–∏ –Ω–∞—Ö–æ–¥–∏–º Article –≤ –ë–î (–Ω—É–∂–µ–Ω –¥–ª—è foreign key)
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è —Å—Ç–∞—Ç—å–∏
        title = source_domain
        h1_match = re.search(r'^#\s+(.+)$', cleaned_content, re.MULTILINE)
        title_match = re.search(r'^Title:\s*(.+)$', cleaned_content, re.MULTILINE)
        
        if h1_match:
            title = h1_match.group(1).strip()
        elif title_match:
            title = title_match.group(1).strip()
        else:
            first_line = next((line.strip() for line in cleaned_content.split('\n') if line.strip() and len(line.strip()) > 10), None)
            if first_line:
                title = first_line[:200]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Å—Ç–∞—Ç—å—è —Å —Ç–∞–∫–∏–º URL
        existing_article = session.query(Article).filter(Article.url == source_url).first()
        
        if existing_article:
            logger.info(f"Using existing article: {existing_article.id}")
            article_id = existing_article.id
        else:
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å—Ç–∞—Ç—å—é –≤ –ë–î
            new_article = Article(
                title=title,
                url=source_url,
                content=cleaned_content,
                source_site=SourceType.URL,
                published_date=None,
                author=source_domain,
                is_processed=False
            )
            
            session.add(new_article)
            session.commit()
            session.refresh(new_article)
            
            article_id = new_article.id
            logger.info(f"Created new article from URL: id={article_id}, title={title[:50]}")

        # –®–∞–≥ 2: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Ç–∞—Ç—å—é —á–µ—Ä–µ–∑ AI (ai_service —É–∂–µ –ø–æ–ª—É—á–µ–Ω –≤—ã—à–µ)

        try:
            article, metrics = await ai_service.generate_article_from_external_content(
                external_content=external_content,
                source_url=source_url,
                source_domain=source_domain,
                project=request.project,
                formatting_options=request.formatting_options
            )

            logger.info(f"Article generated successfully from {source_url}, tokens: {metrics.get('tokens_used', 0)}")

        except Exception as gen_error:
            logger.error(f"AI generation error for {source_url}: {gen_error}")

            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            error_log = GenerationLog(
                draft_id=0,  # –ù–µ—Ç draft_id –ø–æ–∫–∞
                operation_type="url_article_generation",
                model_used="gpt-4o",
                success=False,
                error_message=str(gen_error),
                tokens_used=0,
                processing_time_seconds=0
            )
            session.add(error_log)
            session.commit()

            return URLArticleGenerationResponse(
                success=False,
                draft_id=None,
                source_url=source_url,
                source_domain=source_domain,
                news_text=None,
                seo_title=None,
                seo_description=None,
                seo_keywords=None,
                image_url=None,
                error=f"–û—à–∏–±–∫–∞ AI –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(gen_error)}"
            )

        # –®–∞–≥ 3: –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ—Ä–Ω–æ–≤–∏–∫ –≤ –ë–î
        draft = NewsGenerationDraft(
            article_id=article_id,  # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–π article_id
            project=request.project.value,
            user_id=current_user.id,
            summary=f"–°—Ç–∞—Ç—å—è –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –∏–∑ {source_domain}",
            facts=json.dumps([f"–ò—Å—Ç–æ—á–Ω–∏–∫: {source_url}"]),
            generated_news_text=article.news_text,
            generated_seo_title=article.seo_title,
            generated_seo_description=article.seo_description,
            generated_seo_keywords=json.dumps(article.seo_keywords),
            generated_image_prompt=article.image_prompt,
            generated_image_url=article.image_url if request.generate_image else None,
            status=NewsStatus.GENERATED
        )

        session.add(draft)
        session.commit()
        session.refresh(draft)

        logger.info(f"Draft created: id={draft.id} from URL {source_url}")

        # –®–∞–≥ 4: –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
        gen_log = GenerationLog(
            draft_id=draft.id,
            operation_type="url_article_generation",
            model_used=metrics.get('model_used', 'gpt-4o'),
            success=True,
            error_message=None,
            tokens_used=metrics.get('tokens_used', 0),
            processing_time_seconds=metrics.get('processing_time_seconds', 0)
        )
        session.add(gen_log)
        session.commit()

        return URLArticleGenerationResponse(
            success=True,
            draft_id=draft.id,
            source_url=source_url,
            source_domain=source_domain,
            news_text=article.news_text,
            seo_title=article.seo_title,
            seo_description=article.seo_description,
            seo_keywords=article.seo_keywords,
            image_url=article.image_url if request.generate_image else None,
            error=None
        )

    except HTTPException:
        raise
    except Exception as e:
        session.rollback()
        logger.error(f"Error in generate_article_from_url: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏ –∏–∑ URL: {str(e)}"
        )
